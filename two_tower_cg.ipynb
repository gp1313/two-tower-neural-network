{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23222acc-c98c-4f03-871a-cc9387b09cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 15:09:06.451057: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 15:09:07.527779: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2024-12-05 15:09:07.527910: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2024-12-05 15:09:07.527921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall for CG1:  0.4\n",
      "recall for CG1:  0.2\n",
      "train and test shape : ((4187470, 5), (266364, 5))\n",
      "4187470 266364 4187470 266364 55385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 50.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall evaluation for 1,000 users:\n",
      "Recall @ 100 Candidates: 0.00308\n",
      "Recall @ 1000 Candidates: 0.02244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 52.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall evaluation for 1,000 users:\n",
      "Recall @ 100 Candidates: 0.05095\n",
      "Recall @ 1000 Candidates: 0.20833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 15:10:54.505436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.539570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.543084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.547041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 15:10:54.547969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.551286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.554647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.864783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.867075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.869142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-05 15:10:54.871155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fb4253d8550> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7fb4253d8550>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fb4253d8550> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7fb4253d8550>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 15:10:57.127251: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fb2f645f1f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-05 15:10:57.127291: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-12-05 15:10:57.132130: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-05 15:10:57.244404: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16358/16358 [==============================] - 64s 4ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1251.7834 - regularization_loss: 0.0000e+00 - total_loss: 1251.7834\n",
      "Epoch 2/8\n",
      "16358/16358 [==============================] - 56s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 973.4383 - regularization_loss: 0.0000e+00 - total_loss: 973.4383\n",
      "Epoch 3/8\n",
      "16358/16358 [==============================] - 56s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 774.0133 - regularization_loss: 0.0000e+00 - total_loss: 774.0133\n",
      "Epoch 4/8\n",
      "16358/16358 [==============================] - 55s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 645.0276 - regularization_loss: 0.0000e+00 - total_loss: 645.0276\n",
      "Epoch 5/8\n",
      "16358/16358 [==============================] - 54s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 561.9604 - regularization_loss: 0.0000e+00 - total_loss: 561.9604\n",
      "Epoch 6/8\n",
      "16358/16358 [==============================] - 53s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 505.5219 - regularization_loss: 0.0000e+00 - total_loss: 505.5219\n",
      "Epoch 7/8\n",
      "16358/16358 [==============================] - 54s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 466.2233 - regularization_loss: 0.0000e+00 - total_loss: 466.2233\n",
      "Epoch 8/8\n",
      "16358/16358 [==============================] - 53s 3ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 437.1614 - regularization_loss: 0.0000e+00 - total_loss: 437.1614\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "4/4 [==============================] - 5s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0035 - factorized_top_k/top_5_categorical_accuracy: 0.0115 - factorized_top_k/top_10_categorical_accuracy: 0.0160 - factorized_top_k/top_50_categorical_accuracy: 0.0395 - factorized_top_k/top_100_categorical_accuracy: 0.0560 - loss: 4790.4031 - regularization_loss: 0.0000e+00 - total_loss: 4790.4031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 44.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall evaluation for 1,000 users:\n",
      "Recall @ 100 Candidates: 0.06227\n",
      "Recall @ 1000 Candidates: 0.18906\n",
      "CPU times: user 14min 12s, sys: 2min 1s, total: 16min 13s\n",
      "Wall time: 9min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Text\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "# Read data into memory\n",
    "article_df = pd.read_csv(\"hnmdata/articles.csv\")\n",
    "customer_df = pd.read_csv(\"hnmdata/customers.csv\")\n",
    "transaction_df = pd.read_csv('hnmdata/transactions_train.csv')\n",
    "\n",
    "# Impute missing data\n",
    "customer_df.fillna({\"FN\":0}, inplace=True)\n",
    "customer_df.fillna({\"Active\":0}, inplace=True)\n",
    "customer_df.fillna({\"club_member_status\":\"UNKNOWN\"}, inplace=True)\n",
    "customer_df[\"fashion_news_frequency\"] = customer_df[\"fashion_news_frequency\"].replace({\"None\":\"NONE\"})\n",
    "customer_df.fillna({\"fashion_news_frequency\":\"UNKNOWN\"}, inplace=True)\n",
    "customer_df.fillna({\"age\": customer_df[\"age\"].median()}, inplace=True)\n",
    "\n",
    "# Bucket age groups\n",
    "def create_age_interval(x):\n",
    "    if x <= 25:\n",
    "        return [16, 25]\n",
    "    elif x <= 35:\n",
    "        return [26, 35]\n",
    "    elif x <= 45:\n",
    "        return [36, 45]\n",
    "    elif x <= 55:\n",
    "        return [46, 55]\n",
    "    elif x <= 65:\n",
    "        return [56, 65]\n",
    "    else:\n",
    "        return [66, 99]\n",
    "    \n",
    "customer_df[\"age_interval\"] = customer_df[\"age\"].apply(lambda x: create_age_interval(x))\n",
    "\n",
    "# Make sure all nulls are filled in customer_df\n",
    "assert customer_df.isnull().sum().sum() == 0\n",
    "\n",
    "# Impute missing data\n",
    "article_df.fillna(value=\"No Description\", inplace=True)\n",
    "\n",
    "# Change article_id datatype to string\n",
    "article_df['article_id'] = article_df['article_id'].astype(str)\n",
    "\n",
    "# Add a zero to the left of the article_id string\n",
    "article_df['article_id'] = article_df['article_id'].apply(lambda x: x.zfill(10))\n",
    "\n",
    "transaction_df['article_id'] = transaction_df['article_id'].astype(str)\n",
    "transaction_df['article_id'] = transaction_df['article_id'].apply(lambda x: x.zfill(10))\n",
    "\n",
    "# Intersection helper function\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "# Recall@k = (# of recommended items @k that are relevant) / (total # of relevant items)\n",
    "def estimate_recall(cg, purchase):\n",
    "    return len(intersection(cg,purchase))/len(purchase)\n",
    "\n",
    "\n",
    "# Test the functions with toy example.\n",
    "# How many items from purchases did each CG retrieve?\n",
    "purchases = [\"item45\",\"item97\",\"item71\",\"item125\",\"item5\"]\n",
    "cg1 = [\"item1\",\"item97\",\"item12\",\"item105\",\"item5\",\"item17\",\"item197\",\"item122\",\"item85\",\"item15\"]\n",
    "cg2 = [\"item13\",\"item94\",\"item14\",\"item15\",\"item5\",\"item18\",\"item197\",\"item132\",\"item86\",\"item65\"]\n",
    "\n",
    "print(\"recall for CG1: \", estimate_recall(cg1, purchases))\n",
    "print(\"recall for CG1: \", estimate_recall(cg2, purchases))\n",
    "\n",
    "# Split transactions into train and test\n",
    "N_DAYS_TRAIN = 90\n",
    "N_DAYS_TEST = 7\n",
    "\n",
    "max_date = transaction_df['t_dat'].max()\n",
    "train = transaction_df[(transaction_df['t_dat']>=((pd.to_datetime(max_date) - timedelta(days=N_DAYS_TRAIN+N_DAYS_TEST)).date().strftime('%Y-%m-%d')))\n",
    "                        & (transaction_df['t_dat']<((pd.to_datetime(max_date) - timedelta(days=N_DAYS_TEST)).date().strftime('%Y-%m-%d')))]\n",
    "test = transaction_df[(transaction_df['t_dat']>=((pd.to_datetime(max_date) - timedelta(days=N_DAYS_TEST)).date().strftime('%Y-%m-%d')))]\n",
    "\n",
    "\n",
    "# Delete transaction_df from the namespace to free up some memory\n",
    "transaction_df = None\n",
    "del transaction_df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"train and test shape : {(train.shape, test.shape)}\")\n",
    "\n",
    "c1 = train['customer_id'].to_list()\n",
    "c2 = test['customer_id'].to_list()\n",
    "\n",
    "# Note: Sorting common_users so evaluation is deterministic\n",
    "common_users = sorted(intersection(c1, c2))\n",
    "\n",
    "print(len(train), len(test), len(c1), len(c2), len(common_users))\n",
    "\n",
    "# Pre-calculate unique items so they can be re-used during every invocation\n",
    "train_unique_items = sorted(train['article_id'].unique().tolist())\n",
    "\n",
    "def get_k_candidates_random(u, k):\n",
    "    \"\"\"\n",
    "    Generate k random candidates from the training set.\n",
    "\n",
    "    Args:\n",
    "        u (str): user ID for which to generate the candidates.\n",
    "        k (int): Number of candidates to generate.\n",
    "\n",
    "    Returns:\n",
    "        candidates (list): Random k candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the seed to the user ID to make this function deterministic\n",
    "    random.seed(u)\n",
    "    candidates = random.sample(train_unique_items, k)\n",
    "    return candidates\n",
    "\n",
    "# Pre-calculate item counts so they can be re-used during every invocation\n",
    "train_item_counts = Counter(train['article_id'].to_list()).most_common()\n",
    "\n",
    "def get_top_k_candidates_popular(u, k):\n",
    "    \"\"\"\n",
    "    Generate k most popular (number of times purchased) candidates from the training set.\n",
    "\n",
    "    Args:\n",
    "        u (str): user ID for which to generate the candidates. Not necessary for this function, but is an assumed input during evaluation.\n",
    "        k (int): Number of candidates to generate.\n",
    "\n",
    "    Returns:\n",
    "        candidates (list): The most popular k candidates.\n",
    "    \"\"\"\n",
    "    candidates = [article for article, count in train_item_counts[:k]]\n",
    "    return candidates\n",
    "\n",
    "def run_candidate_generation(method, k_values=[100,1000], user_set_size=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a given candidate generator in terms of recall on the held-out test set.\n",
    "\n",
    "    Args:\n",
    "        method (function): Candidate generation function. User ID (u) and number of candidates (k) arguments.\n",
    "        k_values (list): List of number of candidates to generate and evaluate.\n",
    "        user_set_size (int): Number of users to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        None. Prints results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise evaluation variables\n",
    "    k_values = sorted(k_values)\n",
    "    recall_dict = {k:0 for k in k_values}\n",
    "    user_set = common_users[:user_set_size]\n",
    "\n",
    "    # Loop over users\n",
    "    for u in tqdm(user_set):\n",
    "        # Get list of purchased items for user u\n",
    "        purchase_list = test[test['customer_id']==u]['article_id'].to_list()\n",
    "        # Run candidate generation\n",
    "        cg = method(u, max(k_values))\n",
    "        for k in k_values:\n",
    "            # Estimate recall for candidate generator\n",
    "            recall = estimate_recall(cg[:k], purchase_list)\n",
    "            # Add this to overall recall (to be averaged at end)\n",
    "            recall_dict[k] += recall\n",
    "\n",
    "    print(f\"\\nRecall evaluation for {user_set_size:,} users:\")\n",
    "    for k in k_values:\n",
    "        # Average recall by dividing sum of recalls by user_set length\n",
    "        overall_recall = recall_dict[k] / len(user_set)\n",
    "        print(f\"Recall @ {k} Candidates: {overall_recall:.5f}\")\n",
    "# Evaluate CG 1 (random)\n",
    "run_candidate_generation(get_k_candidates_random)\n",
    "\n",
    "# Evaluate CG 2 (popularity)\n",
    "run_candidate_generation(get_top_k_candidates_popular)\n",
    "\n",
    "# Create lists of unique ids\n",
    "unique_customer_ids = train.customer_id.unique()\n",
    "unique_article_ids = train.article_id.unique()\n",
    "\n",
    "embedding_dimension = 128\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "customer_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_customer_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_customer_ids) + 1, embedding_dimension),\n",
    "  tf.keras.layers.Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "article_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_article_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_article_ids) + 1, embedding_dimension),\n",
    "  tf.keras.layers.Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "article_ds = tf.data.Dataset.from_tensor_slices(dict(article_df[['article_id']]))\n",
    "articles = article_ds.map(lambda x: x['article_id'])\n",
    "\n",
    "class HandMModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, customer_model, article_model):\n",
    "        super().__init__()\n",
    "        self.article_model: tf.keras.Model = article_model\n",
    "        self.customer_model: tf.keras.Model = customer_model\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=articles.batch(128).map(self.article_model), # Batching articles into size 128 and passing through the article model\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[str, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        customer_embeddings = self.customer_model(features[\"customer_id\"])\n",
    "        article_embeddings = self.article_model(features[\"article_id\"])\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        # Note that by default compute_metrics is set to not_training as running during training is VERY expensive\n",
    "        return self.task(customer_embeddings, article_embeddings, compute_metrics=not training)\n",
    "# Instantiate model\n",
    "model = HandMModel(customer_model, article_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# The `from_tensor_slices` method creates a dataset with a separate element for each row of the input tensor\n",
    "# Shuffling randomly shuffles the dataset and batching sets the batch size to 256\n",
    "# Caching keeps the dataset in memory (or a specified file). For larger datasets sometimes we cannot fit the entire dataset in memory and thus use a file on disk.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(train[['customer_id','article_id']])).shuffle(100_000, seed=42).batch(256).cache()\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "# FactorizedTopK will show as 0, but these are the expensive metrics and as training=True they will not be calculated\n",
    "num_epochs = 8\n",
    "history = model.fit(train_ds, epochs=num_epochs, verbose=1)\n",
    "\n",
    "# Take a sample of 2000 elements from the test set and run inference on them\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(test.sample(2000, random_state=42)[['customer_id','article_id']])).batch(500).cache()\n",
    "\n",
    "# We now see the metrics are not in fact zero (train=False here, so they are computed)\n",
    "model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.customer_model)\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((articles.batch(1000), articles.batch(1000).map(model.article_model)))\n",
    ")\n",
    "\n",
    "def get_top_k_candidates_2_tower(u, k):\n",
    "    \"\"\"\n",
    "    Generate k candidates from the training set using the two-tower model.\n",
    "\n",
    "    Args:\n",
    "        u (str): user ID for which to generate the candidates.\n",
    "        k (int): Number of candidates to generate.\n",
    "\n",
    "    Returns:\n",
    "        candidates (list): The top-k candidates.\n",
    "    \"\"\"\n",
    "    _, candidates = index(tf.constant([u]), k=k)\n",
    "    candidates = candidates.numpy().flatten()\n",
    "    candidates = [c.decode(\"utf-8\") for c in candidates]\n",
    "    return candidates\n",
    "\n",
    "run_candidate_generation(get_top_k_candidates_2_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b6e09-1e6f-47a0-a785-99d02b2631af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
